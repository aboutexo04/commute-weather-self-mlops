version: '3.8'

services:
  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: mlops-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro  # SSL certificates
    depends_on:
      - mlops-app
    networks:
      - mlops-network
    restart: unless-stopped

  # MLOps 메인 애플리케이션
  mlops-app:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: commute-weather-mlops-prod
    ports:
      - "8000:8000"
    environment:
      - PYTHONPATH=/app/src
      - ENVIRONMENT=production
    volumes:
      - ./.env:/app/.env:ro  # Production 환경 변수
      - ./logs:/app/logs     # 로그 디렉토리
      - ./mlflow:/app/mlflow # MLflow 로컬 저장소 (백업용)
    depends_on:
      - redis
    networks:
      - mlops-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Redis (캐싱 및 작업 큐)
  redis:
    image: redis:7-alpine
    container_name: mlops-redis-prod
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - mlops-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'

  # MLflow 서버 (S3 기반)
  mlflow-server:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: mlops-mlflow-prod
    ports:
      - "5001:5000"
    environment:
      - PYTHONPATH=/app/src
    volumes:
      - ./.env:/app/.env:ro
      - ./mlflow:/app/mlflow
    command: ["mlflow", "server", "--host", "0.0.0.0", "--port", "5000", "--backend-store-uri", "file:///app/mlflow", "--default-artifact-root", "s3://${COMMUTE_S3_BUCKET}/mlflow-artifacts"]
    networks:
      - mlops-network
    restart: unless-stopped

  # 데이터 수집기 (스케줄된 작업)
  data-collector:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: mlops-collector-prod
    environment:
      - PYTHONPATH=/app/src
      - ENVIRONMENT=production
    volumes:
      - ./.env:/app/.env:ro
      - ./logs:/app/logs
      - ./data:/app/data
    command: ["python", "scripts/run_realtime_mlops.py"]
    depends_on:
      - redis
      - mlops-app
    networks:
      - mlops-network
    restart: unless-stopped

  # 모델 훈련 스케줄러
  training-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: mlops-trainer-prod
    environment:
      - PYTHONPATH=/app/src
      - ENVIRONMENT=production
    volumes:
      - ./.env:/app/.env:ro
      - ./logs:/app/logs
      - ./data:/app/data
    command: ["python", "scripts/run_training_scheduler.py"]
    depends_on:
      - redis
      - mlflow-server
    networks:
      - mlops-network
    restart: unless-stopped

  # 모니터링 (Prometheus)
  prometheus:
    image: prom/prometheus:latest
    container_name: mlops-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - mlops-network
    restart: unless-stopped

  # 시각화 (Grafana)
  grafana:
    image: grafana/grafana:latest
    container_name: mlops-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    networks:
      - mlops-network
    restart: unless-stopped

  # Airflow PostgreSQL (Airflow 메타데이터)
  airflow-postgres:
    image: postgres:13
    container_name: mlops-airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    networks:
      - mlops-network
    restart: unless-stopped

  # Airflow 웹서버
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: mlops-airflow-webserver
    user: root
    command: bash -c "pip install psycopg2-binary && airflow standalone"
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/app/dags
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE=Asia/Seoul
      - AIRFLOW__WEBSERVER__RBAC=False
      - AIRFLOW__WEBSERVER__AUTHENTICATE=False
      - AIRFLOW__CORE__FERNET_KEY=fernet_key_here_32_character_string
      - PYTHONPATH=/app/src
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=ap-northeast-2
      - KMA_AUTH_KEY=${KMA_AUTH_KEY}
      - COMMUTE_S3_BUCKET=${COMMUTE_S3_BUCKET}
      - WANDB_API_KEY=${WANDB_API_KEY}
    volumes:
      - ./.env:/app/.env:ro
      - ./dags:/app/dags
      - ./logs:/app/logs
      - ./airflow:/app/airflow
    depends_on:
      - airflow-postgres
      - redis
    networks:
      - mlops-network
    restart: unless-stopped

  # Airflow 스케줄러
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: mlops-airflow-scheduler
    user: root
    command: bash -c "pip install psycopg2-binary && airflow scheduler"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      - AIRFLOW__CORE__DAGS_FOLDER=/app/dags
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__FERNET_KEY=fernet_key_here_32_character_string
      - PYTHONPATH=/app/src
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=ap-northeast-2
      - KMA_AUTH_KEY=${KMA_AUTH_KEY}
      - COMMUTE_S3_BUCKET=${COMMUTE_S3_BUCKET}
      - WANDB_API_KEY=${WANDB_API_KEY}
    volumes:
      - ./.env:/app/.env:ro
      - ./dags:/app/dags
      - ./logs:/app/logs
      - ./airflow:/app/airflow
    depends_on:
      - airflow-postgres
      - redis
    networks:
      - mlops-network
    restart: unless-stopped

networks:
  mlops-network:
    driver: bridge

volumes:
  redis-data:
  prometheus-data:
  grafana-data:
  airflow-postgres-data: